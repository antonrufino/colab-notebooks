{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Style Transfer",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOUB75w14JFHZJGuHZ/rsno",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antonrufino/colab-notebooks/blob/main/Neural_Style_Transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXgcdgX9dAAw"
      },
      "source": [
        "An implementation of [Image Style Transfer](https://openaccess.thecvf.com/content_cvpr_2016/html/Gatys_Image_Style_Transfer_CVPR_2016_paper.html) in PyTorch. The implementation is based on the [tutorial](https://pytorch.org/tutorials/advanced/neural_style_tutorial.html) provided by PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6GDZGPoX7iu"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNS0KKDHX6tE"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision.models import vgg19\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZIYgmIO0Fom"
      },
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUDQ_rMNyAhG"
      },
      "source": [
        "EPOCHS = 300\n",
        "NUM_CHANNELS = 3\n",
        "IMG_HEIGHT = 800\n",
        "IMG_WIDTH = 800\n",
        "ALPHA = 1e-3\n",
        "BETA = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAC7SbJvnBl5"
      },
      "source": [
        "# Input\n",
        "\n",
        "Use only square shaped photos i.e. photos with the same width and height."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJgv8MmYnX9v"
      },
      "source": [
        "def load_img():\n",
        "  upload_dict = files.upload()\n",
        "  img_path = list(upload_dict.keys())[0]\n",
        "  return Image.open(img_path).convert('RGB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9o8ZvFYBnRFd"
      },
      "source": [
        "## Style Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e05KT8DynWWk"
      },
      "source": [
        "#style_img = load_img()\n",
        "style_img = Image.open('/path/to/style/img')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB4grOfqtIDx"
      },
      "source": [
        "## Content Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85OIVO72tsnN"
      },
      "source": [
        "#content_img = load_img()\n",
        "content_img = Image.open('/path/to/style/img')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcH1VgsoRLGP"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwaNwCGGRVBq"
      },
      "source": [
        "pipeline = transforms.Compose([\n",
        "  transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n",
        "  transforms.ToTensor()\n",
        "])\n",
        "\n",
        "style_tensor = pipeline(style_img).unsqueeze(0)\n",
        "content_tensor = pipeline(content_img).unsqueeze(0)\n",
        "\n",
        "print(style_tensor)\n",
        "print(content_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fmh8cGA8Yjd4"
      },
      "source": [
        "# Loss Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQ8NL6NstZdg"
      },
      "source": [
        "## Style Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_rkVc18tbFD"
      },
      "source": [
        "class StyleLoss(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(StyleLoss, self).__init__()\n",
        "\n",
        "  def gram_matrix(self, tensor):\n",
        "    n, c, h, w = tensor.shape\n",
        "    tensor = tensor.view(n * c, h * w)\n",
        "\n",
        "    return torch.mm(tensor, tensor.t())\n",
        "\n",
        "  def forward(self, input, target):\n",
        "    n, c, h, w = input.shape\n",
        "\n",
        "    input = self.gram_matrix(input)\n",
        "    target = self.gram_matrix(target)\n",
        "    return F.mse_loss(input, target) / 4.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzMjssQdYnvy"
      },
      "source": [
        "## Content Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDb5_9MvYhdl"
      },
      "source": [
        "class ContentLoss(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ContentLoss, self).__init__()\n",
        "\n",
        "  def forward(self, input, target):\n",
        "    return F.mse_loss(input, target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqgKHIqHPbZ5"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSoZL2yyPfPl"
      },
      "source": [
        "vgg_model = vgg19(pretrained=True)\n",
        "print(vgg_model)\n",
        "\n",
        "class NeuralStyleTransfer(nn.Module):\n",
        "  def __init__(self, vgg_model):\n",
        "    super(NeuralStyleTransfer, self).__init__()\n",
        "\n",
        "    feature_extractor = vgg_model.features\n",
        "\n",
        "    # Replace max pooling with avg pooling\n",
        "\n",
        "    feature_extractor[4] = nn.AvgPool2d(kernel_size=2, stride=2, padding=0, ceil_mode=False)\n",
        "    feature_extractor[9] = nn.AvgPool2d(kernel_size=2, stride=2, padding=0, ceil_mode=False)\n",
        "    feature_extractor[18] = nn.AvgPool2d(kernel_size=2, stride=2, padding=0, ceil_mode=False)\n",
        "    feature_extractor[27] = nn.AvgPool2d(kernel_size=2, stride=2, padding=0, ceil_mode=False)\n",
        "\n",
        "    # Isolate portions of network that produce needed activations\n",
        "\n",
        "    self.conv1_1 = feature_extractor[:2]\n",
        "    self.conv2_1 = feature_extractor[2:7]\n",
        "    self.conv3_1 = feature_extractor[7:12]\n",
        "    self.conv4_1 = feature_extractor[12:21]\n",
        "    self.conv5_1 = feature_extractor[21:30]\n",
        "\n",
        "  def forward(self, x):\n",
        "    a = self.conv1_1(x)\n",
        "    b = self.conv2_1(a)\n",
        "    c = self.conv3_1(b)\n",
        "    d = self.conv4_1(c)\n",
        "    e = self.conv5_1(d)\n",
        "\n",
        "    return [a, b, c, d, e]\n",
        "\n",
        "nst_model = NeuralStyleTransfer(vgg_model).to(device)\n",
        "print(nst_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILP_JUYE4FNK"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VV3CS-23y6H"
      },
      "source": [
        "with torch.no_grad():\n",
        "  style_tensor = style_tensor.to(device)\n",
        "  content_tensor = content_tensor.to(device)\n",
        "\n",
        "  style_activations = nst_model(style_tensor)\n",
        "  content_activations = nst_model(content_tensor)\n",
        "\n",
        "white_noise_img = content_tensor.clone() #torch.rand(1, NUM_CHANNELS, IMG_HEIGHT, IMG_WIDTH)\n",
        "white_noise_img = white_noise_img.to(device)\n",
        "white_noise_img = white_noise_img.requires_grad_()\n",
        "\n",
        "style_criterion = StyleLoss()\n",
        "content_criterion = ContentLoss()\n",
        "optimizer = optim.Adam([white_noise_img], lr=3e-2)\n",
        "\n",
        "for i in range(EPOCHS):\n",
        "  white_noise_img.data.clamp_(0, 1)\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  activations = nst_model(white_noise_img)\n",
        "  \n",
        "  style_loss = 0.0\n",
        "  for j in range(len(activations)):\n",
        "    style_loss += style_criterion(activations[j], style_activations[j])\n",
        "  \n",
        "  content_loss = content_criterion(activations[3], content_activations[3])\n",
        "\n",
        "  loss = ALPHA * content_loss + BETA * style_loss\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if ((i+1) % 50 == 0):\n",
        "    print(loss.item())\n",
        "\n",
        "white_noise_img = white_noise_img.cpu()\n",
        "white_noise_img.clamp_(0, 1)\n",
        "white_noise_img.squeeze_(0)\n",
        "\n",
        "convert_to_img = transforms.ToPILImage()\n",
        "final_img = convert_to_img(white_noise_img)\n",
        "final_img"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}