{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CycleGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMnLPewNkP3yMrxMaykAzI+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antonrufino/colab-notebooks/blob/main/CycleGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bcmzAonA368"
      },
      "source": [
        "An implementation of CycleGAN from [Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks](https://arxiv.org/abs/1703.10593). The models have been modified to fit the constraints of Google Colab's GPUs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AehQ74n29Jbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7371452d-1586-442d-e5f9-d28e859f1e4b"
      },
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.utils import data\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid, save_image\n",
        "\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6_hgnYC2DxS"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxZjYtZ7FCDs"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqR66XjlFElG"
      },
      "source": [
        "## Residual Block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wn4U9Xw7E-v6"
      },
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ResidualBlock, self).__init__()\n",
        "  \n",
        "    self.conv1 = nn.Conv2d(256, 256, 3, padding=1)\n",
        "    self.bn1 = nn.InstanceNorm2d(256)\n",
        "    self.conv2 = nn.Conv2d(256, 256, 3, padding=1)\n",
        "    self.bn2 = nn.InstanceNorm2d(256)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    tmp = self.conv1(x)\n",
        "    tmp = self.bn1(tmp)\n",
        "    tmp = F.relu(tmp)\n",
        "\n",
        "    tmp = self.conv2(tmp)\n",
        "    tmp = self.bn2(tmp)\n",
        "    y = x + tmp\n",
        "    y = F.relu(y)\n",
        "\n",
        "    return y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8GUfVBvP0Tt"
      },
      "source": [
        "## Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcBTlfXzP7CT"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Generator, self).__init__()\n",
        "\n",
        "    self.down = nn.Sequential(\n",
        "        nn.ReflectionPad2d(3),\n",
        "        nn.Conv2d(3, 64, 7),\n",
        "        nn.InstanceNorm2d(64),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Conv2d(64, 128, 3, 2, padding=1),\n",
        "        nn.InstanceNorm2d(128),\n",
        "        nn.ReLU(),\n",
        "        \n",
        "        nn.Conv2d(128, 256, 3, 2, padding=1),\n",
        "        nn.InstanceNorm2d(256),\n",
        "        nn.ReLU(),   \n",
        "    )\n",
        "\n",
        "    self.body = nn.Sequential(*[ResidualBlock() for _ in range(6)])\n",
        "\n",
        "    self.up = nn.Sequential(\n",
        "        nn.ConvTranspose2d(256, 128, 3, 2, padding=1, output_padding=1),\n",
        "        nn.InstanceNorm2d(128),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.ConvTranspose2d(128, 64, 3, 2, padding=1, output_padding=1),\n",
        "        nn.InstanceNorm2d(64),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.ReflectionPad2d(3),\n",
        "        nn.Conv2d(64, 3, 7),\n",
        "        nn.InstanceNorm2d(3),\n",
        "        # nn.ReLU(),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.down(x)\n",
        "    x = self.body(x)\n",
        "    x = self.up(x)\n",
        "    x = torch.tanh(x)\n",
        "    \n",
        "    x = (x + 1) / 2 # Limit output to [0,1]\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFTvw0YMMMsZ"
      },
      "source": [
        "## Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_2i6cIhMMKH"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Discriminator, self).__init__()\n",
        "\n",
        "    self.network = nn.Sequential(\n",
        "        nn.Conv2d(3, 64, 4, 2),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        nn.Conv2d(64, 128, 4, 2),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        nn.Conv2d(128, 256, 4, 2),\n",
        "        nn.BatchNorm2d(256),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        nn.Conv2d(256, 512, 4, 2),\n",
        "        nn.BatchNorm2d(512),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        nn.Conv2d(512, 1, 4)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.network(x)\n",
        "    x = torch.sigmoid(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F200T3sW62oO"
      },
      "source": [
        "# Losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxTC8JKZ64w1"
      },
      "source": [
        "class DiscriminatorLoss(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(DiscriminatorLoss, self).__init__()\n",
        "\n",
        "  def forward(self, d, x_fake, x_real):\n",
        "    return (torch.mean((d(x_real) - 1) ** 2) + torch.mean(d(x_fake) ** 2)) / 2\n",
        "\n",
        "class GeneratorLoss(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(GeneratorLoss, self).__init__()\n",
        "\n",
        "  def forward(self, g, d, x):\n",
        "    return torch.mean((d(g(x)) - 1) ** 2)\n",
        "\n",
        "class CycleLoss(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CycleLoss, self).__init__()\n",
        "\n",
        "  def forward(self, g, f, x, y):\n",
        "    total = torch.mean((f(g(x)) - x) ** 2) \n",
        "    total += torch.mean((g(f(y)) - y) ** 2)\n",
        "    return total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wdTkx0RrXw2"
      },
      "source": [
        "# Style Transfer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjWWppkSrdYv"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLa1c_uh12A7"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/junyanz/pytorch-CycleGAN-and-pix2pix/master/datasets/download_cyclegan_dataset.sh\n",
        "!mkdir datasets\n",
        "!bash ./download_cyclegan_dataset.sh vangogh2photo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpeV9FTDrfs0"
      },
      "source": [
        "class ImageDataset(data.Dataset):\n",
        "  def __init__(self, img_dir, height, width):\n",
        "    self.img_dir = img_dir\n",
        "    self.file_list = os.listdir(img_dir)\n",
        "    self.pipeline = transforms.Compose([\n",
        "      transforms.Resize((height, width)),\n",
        "      transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.file_list)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img = Image.open(os.path.join(self.img_dir, self.file_list[idx]))\n",
        "    img = self.pipeline(img)\n",
        "    return img.to(device)\n",
        "\n",
        "x_dataset = ImageDataset(\n",
        "    img_dir='./datasets/vangogh2photo/trainB',\n",
        "    height=128,\n",
        "    width=128)\n",
        "\n",
        "y_dataset = ImageDataset(\n",
        "    img_dir='./datasets/vangogh2photo/trainA',\n",
        "    height=128,\n",
        "    width=128)\n",
        "\n",
        "x_dataloader = data.DataLoader(x_dataset, batch_size=1, shuffle=True)\n",
        "y_dataloader = data.DataLoader(y_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "print(x_dataset[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRNCBk1Z8Tnj"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKo_G1qHLn1H"
      },
      "source": [
        "EPOCHS = 20\n",
        "LAMBDA = 10\n",
        "IMG_HEIGHT = 128\n",
        "IMG_WIDTH = 128\n",
        "\n",
        "TRAIN_MODEL = True\n",
        "TENSORBOARD_DIR = 'logs/test1'\n",
        "PRETRAINED_G_MODEL_PATH = 'pretrained-g.pt'\n",
        "PRETRAINED_F_MODEL_PATH = 'pretrained-f.pt'\n",
        "PRETRAINED_DX_MODEL_PATH = 'pretrained-dx.pt'\n",
        "PRETRAINED_DY_MODEL_PATH = 'pretrained-dy.pt'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRrCzYRW5zDJ"
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0Bf-BoRAuFW"
      },
      "source": [
        "# !kill 1205"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-e4Lz4Cc8XKM"
      },
      "source": [
        "if TRAIN_MODEL:\n",
        "  g = Generator().to(device)\n",
        "  f = Generator().to(device)\n",
        "  d_x = Discriminator().to(device)\n",
        "  d_y = Discriminator().to(device)\n",
        "\n",
        "  print(g(torch.randn(2, 3, 100, 100).to(device)).shape)\n",
        "  print(f(torch.randn(2, 3, 100, 100).to(device)).shape)\n",
        "  print(d_x(torch.randn(2, 3, 100, 100).to(device)).shape)\n",
        "  print(d_y(torch.randn(2, 3, 100, 100).to(device)).shape)\n",
        "\n",
        "  loss_gen = GeneratorLoss()\n",
        "  loss_dis = DiscriminatorLoss()\n",
        "  loss_cyc = CycleLoss()\n",
        "\n",
        "  adam_g = optim.Adam(g.parameters(), 2e-4)\n",
        "  adam_f = optim.Adam(f.parameters(), 2e-4)\n",
        "  adam_d_x = optim.Adam(d_x.parameters(), 2e-4)\n",
        "  adam_d_y = optim.Adam(d_y.parameters(), 2e-4)\n",
        "\n",
        "  g_history = []\n",
        "  f_history = []\n",
        "\n",
        "  writer = SummaryWriter('logs/test1')\n",
        "\n",
        "  running_loss_dis = 0.0\n",
        "  running_loss_gen = 0.0\n",
        "\n",
        "  test_img = Image.open('/content/datasets/vangogh2photo/testB/2014-08-01 17:41:55.jpg')\n",
        "  pipeline = transforms.Compose([\n",
        "    transforms.Resize(128),\n",
        "    transforms.ToTensor()\n",
        "  ])\n",
        "\n",
        "  test_tensor = pipeline(test_img).unsqueeze(0).to(device)\n",
        "\n",
        "  count = 0\n",
        "  for i in range(EPOCHS):\n",
        "    x_iter = iter(x_dataloader)\n",
        "    y_iter = iter(y_dataloader)\n",
        "\n",
        "    while True:\n",
        "      try:\n",
        "        x_sample = next(x_iter)\n",
        "        y_sample = next(y_iter)\n",
        "      except StopIteration:\n",
        "        break\n",
        "\n",
        "      count += 1\n",
        "\n",
        "      adam_d_x.zero_grad()\n",
        "      adam_d_y.zero_grad()\n",
        "\n",
        "      g_history.append(g(x_sample))\n",
        "      f_history.append(f(y_sample))\n",
        "\n",
        "      if len(g_history) == 51:\n",
        "        g_history = g_history[1:]\n",
        "\n",
        "      if len(f_history) == 51:\n",
        "        f_history = f_history[1:]\n",
        "\n",
        "      y_stacked = torch.cat(g_history, 0).detach()\n",
        "      x_stacked = torch.cat(f_history, 0).detach()\n",
        "\n",
        "      loss = loss_dis(d_y, y_stacked, y_sample)\n",
        "      loss += loss_dis(d_x, x_stacked, x_sample)\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      running_loss_dis += loss.item()\n",
        "\n",
        "      adam_d_x.step()\n",
        "      adam_d_y.step()\n",
        "\n",
        "      adam_g.zero_grad()\n",
        "      adam_f.zero_grad()\n",
        "\n",
        "      loss = loss_gen(g, d_y, x_sample)\n",
        "      loss += loss_gen(f, d_x, y_sample)\n",
        "      loss += LAMBDA * loss_cyc(g, f, x_sample, y_sample)\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      running_loss_gen += loss.item()\n",
        "\n",
        "      adam_g.step()\n",
        "      adam_f.step()\n",
        "\n",
        "      if count % 100 == 0:\n",
        "        writer.add_scalar('discriminator loss', running_loss_dis / 100, count)\n",
        "        writer.add_scalar('generator loss', running_loss_gen / 100, count)\n",
        "\n",
        "        running_loss_dis = 0.0\n",
        "        running_loss_gen = 0.0\n",
        "\n",
        "        pic_dis = d_x(test_tensor)[0]\n",
        "        writer.add_scalar('discriminator output on test image', torch.mean(pic_dis), count)\n",
        "\n",
        "        torch.save(g.state_dict(), PRETRAINED_G_MODEL_PATH)\n",
        "        torch.save(f.state_dict(), PRETRAINED_F_MODEL_PATH)\n",
        "        torch.save(d_x.state_dict(), PRETRAINED_DX_MODEL_PATH)\n",
        "        torch.save(d_y.state_dict(), PRETRAINED_DY_MODEL_PATH)\n",
        "\n",
        "      if count % 100 == 0:\n",
        "        pic =  g(test_tensor)\n",
        "        grid = make_grid([test_tensor[0], pic[0], f(pic)[0]])\n",
        "\n",
        "        writer.add_image('generated sample', grid, global_step=count)\n",
        "\n",
        "  torch.save(g.state_dict(), PRETRAINED_G_MODEL_PATH)\n",
        "  torch.save(f.state_dict(), PRETRAINED_F_MODEL_PATH)\n",
        "  torch.save(d_x.state_dict(), PRETRAINED_DX_MODEL_PATH)\n",
        "  torch.save(d_y.state_dict(), PRETRAINED_DY_MODEL_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-0_ZsgDG1rH"
      },
      "source": [
        "## Run pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQbB8ES1G07V"
      },
      "source": [
        "# INPUT_IMG_PATH = '/content/datasets/vangogh2photo/testA/00001.jpg'\n",
        "\n",
        "# g = Generator().to(device)\n",
        "# g.load_state_dict(torch.load(PRETRAINED_G_MODEL_PATH))\n",
        "\n",
        "# input_img = Image.open(INPUT_IMG_PATH)\n",
        "# input_img = transforms.ToTensor()(input_img).to(device).unsqueeze(0)\n",
        "# output_img = g(input_img)[0]\n",
        "# save_image(output_img, './output.png')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}